{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install econml","metadata":{"execution":{"iopub.status.busy":"2022-04-29T13:24:27.318126Z","iopub.execute_input":"2022-04-29T13:24:27.318535Z","iopub.status.idle":"2022-04-29T13:24:50.680423Z","shell.execute_reply.started":"2022-04-29T13:24:27.318422Z","shell.execute_reply":"2022-04-29T13:24:50.679361Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"# External Libaries required for the majority of the assignment, more to be added as more tasks are completed \nfrom econml.metalearners import XLearner\nfrom sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.model_selection import GridSearchCV\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport scipy.stats as st\nimport matplotlib.pyplot as plt","metadata":{"execution":{"iopub.status.busy":"2022-04-29T13:24:50.682975Z","iopub.execute_input":"2022-04-29T13:24:50.683326Z","iopub.status.idle":"2022-04-29T13:25:00.846197Z","shell.execute_reply.started":"2022-04-29T13:24:50.683281Z","shell.execute_reply":"2022-04-29T13:25:00.845561Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"class Metrics:\n    \n    def pehe(self,effect_true, effect_pred):\n        \"\"\"\n        Precision in Estimating the Heterogeneous Treatment Effect (PEHE)\n        :param effect_true: true treatment effect value\n        :param effect_pred: predicted treatment effect value\n        :return: PEHE\n        \"\"\"\n        return np.abs(np.mean(effect_pred) - np.mean(effect_true))\n\n    def abs_ate(self,effect_true, effect_pred):\n        \"\"\"\n        Absolute error for the Average Treatment Effect (ATE)\n        :param effect_true: true treatment effect value\n        :param effect_pred: predicted treatment effect value\n        :return: absolute error on ATE\n        \"\"\"\n        return np.sqrt(np.mean((effect_true - effect_pred)**2))\n    @staticmethod\n    def abs_att(effect_pred, yf, t, e):\n        \"\"\"\n        Absolute error for the Average Treatment Effect on the Treated\n        :param effect_pred: predicted treatment effect value\n        :param yf: factual (observed) outcome\n        :param t: treatment status (treated/control)\n        :param e: whether belongs to the experimental group\n        :return: absolute error on ATT\n        \"\"\"\n        att_true = np.mean(yf[t > 0]) - np.mean(yf[(1 - t + e) > 1])\n        att_pred = np.mean(effect_pred[(t + e) > 1])\n\n        return np.abs(att_pred - att_true)\n    @staticmethod\n    def policy_risk(effect_pred, yf, t, e):\n        \"\"\"\n        Computes the risk of the policy defined by predicted effect\n        :param effect_pred: predicted treatment effect value\n        :param yf: factual (observed) outcome\n        :param t: treatment status (treated/control)\n        :param e: whether belongs to the experimental group\n        :return: policy risk\n        \"\"\"\n        # Consider only the cases for which we have experimental data (i.e., e > 0)\n        t_e = t[e > 0]\n        yf_e = yf[e > 0]\n        effect_pred_e = effect_pred[e > 0]\n\n        if np.any(np.isnan(effect_pred_e)):\n            return np.nan\n\n        policy = effect_pred_e > 0.0\n        treat_overlap = (policy == t_e) * (t_e > 0)\n        control_overlap = (policy == t_e) * (t_e < 1)\n\n        if np.sum(treat_overlap) == 0:\n            treat_value = 0\n        else:\n            treat_value = np.mean(yf_e[treat_overlap])\n\n        if np.sum(control_overlap) == 0:\n            control_value = 0\n        else:\n            control_value = np.mean(yf_e[control_overlap])\n\n        pit = np.mean(policy)\n        policy_value = pit * treat_value + (1.0 - pit) * control_value\n\n        return 1.0 - policy_value\nmetrics = Metrics()","metadata":{"execution":{"iopub.status.busy":"2022-04-29T13:25:00.847511Z","iopub.execute_input":"2022-04-29T13:25:00.848310Z","iopub.status.idle":"2022-04-29T13:25:00.862887Z","shell.execute_reply.started":"2022-04-29T13:25:00.848274Z","shell.execute_reply":"2022-04-29T13:25:00.861706Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"## Data Exploration, Preprocessing and Modelling","metadata":{}},{"cell_type":"code","source":"# x =  Background Variables, t = Treatment Variable (Support or no support), yf = Outcome Variable (Factual)\n# ycf = Outcome Variable (Counterfactual), ite = individual treatment effect\ndata = np.load('../input/datasetihdp/ihdp.npz')\nfor f in data.files:\n  print(f'{f}: {data[f].shape}')","metadata":{"execution":{"iopub.status.busy":"2022-04-29T13:25:00.864850Z","iopub.execute_input":"2022-04-29T13:25:00.865255Z","iopub.status.idle":"2022-04-29T13:25:00.961842Z","shell.execute_reply.started":"2022-04-29T13:25:00.865225Z","shell.execute_reply":"2022-04-29T13:25:00.961192Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"df_x,df_t,df_yf,df_ycf,df_ite = data['x'],data['t'],data['yf'], data['ycf'],data['ite']\nprint('ATE : ', np.mean(df_ite))","metadata":{"execution":{"iopub.status.busy":"2022-04-29T13:25:00.963096Z","iopub.execute_input":"2022-04-29T13:25:00.963513Z","iopub.status.idle":"2022-04-29T13:25:00.971993Z","shell.execute_reply.started":"2022-04-29T13:25:00.963482Z","shell.execute_reply":"2022-04-29T13:25:00.971346Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"IHDP_x,IHDP_t,IHDP_yf,IHDP_ycf, IHDP_ite = pd.DataFrame(df_x),pd.DataFrame(df_t), pd.DataFrame(df_yf), pd.DataFrame(df_ycf), pd.DataFrame(df_ite)\nIHDP_x.info()","metadata":{"execution":{"iopub.status.busy":"2022-04-29T13:25:00.973447Z","iopub.execute_input":"2022-04-29T13:25:00.973874Z","iopub.status.idle":"2022-04-29T13:25:01.006262Z","shell.execute_reply.started":"2022-04-29T13:25:00.973843Z","shell.execute_reply":"2022-04-29T13:25:01.005629Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"There appears to be no missing data or non-numerical values from the IHDP dataset therefore no preprocessing is needed in regards to encoding and filling Nan rows.","metadata":{}},{"cell_type":"code","source":"IHDP_x.describe().T","metadata":{"execution":{"iopub.status.busy":"2022-04-29T13:25:01.007565Z","iopub.execute_input":"2022-04-29T13:25:01.007775Z","iopub.status.idle":"2022-04-29T13:25:01.093717Z","shell.execute_reply.started":"2022-04-29T13:25:01.007750Z","shell.execute_reply":"2022-04-29T13:25:01.093070Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"sns.pairplot(data=IHDP_x)","metadata":{"execution":{"iopub.status.busy":"2022-04-29T13:25:01.094585Z","iopub.execute_input":"2022-04-29T13:25:01.094905Z","iopub.status.idle":"2022-04-29T13:27:52.911199Z","shell.execute_reply.started":"2022-04-29T13:25:01.094878Z","shell.execute_reply":"2022-04-29T13:27:52.907691Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"bins=20\nfig, axs = plt.subplots(1, 5, figsize=(16, 4))\naxs[0].hist(df_x, bins=bins)\naxs[1].hist(df_t, bins=bins)\naxs[2].hist(df_yf, bins=bins)\naxs[3].hist(df_ycf, bins=bins)\naxs[4].hist(df_ite, bins=bins)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-29T13:27:52.914544Z","iopub.execute_input":"2022-04-29T13:27:52.914896Z","iopub.status.idle":"2022-04-29T13:27:54.543278Z","shell.execute_reply.started":"2022-04-29T13:27:52.914853Z","shell.execute_reply":"2022-04-29T13:27:54.542461Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"IHDP_x.hist(bins=25,figsize=(12,10))","metadata":{"execution":{"iopub.status.busy":"2022-04-29T13:27:54.547135Z","iopub.execute_input":"2022-04-29T13:27:54.547790Z","iopub.status.idle":"2022-04-29T13:27:58.835171Z","shell.execute_reply.started":"2022-04-29T13:27:54.547757Z","shell.execute_reply":"2022-04-29T13:27:58.834345Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"limit = 150\nfig, axs = plt.subplots(1, 2, figsize=(16, 4))\n# These scatterplots have only been made via factual outcomes \n# More scatterplots could be made to model the counterfactual outcomes\naxs[0].scatter(df_x[:, 0].reshape(-1, 1)[df_t == 1][:limit]\n               , df_yf[df_t == 1][:limit], label = \"Treated\")\naxs[0].scatter(df_x[:, 0].reshape(-1, 1)[df_t == 0][:limit]\n               , df_yf[df_t == 0][:limit], label = \"Control\")\naxs[1].scatter(df_x[:, 0].reshape(-1, 1)[df_t == 1][:limit]\n               , df_ycf[df_t == 1][:limit], label = \"Treated\")\naxs[1].scatter(df_x[:, 0].reshape(-1, 1)[df_t == 0][:limit]\n               , df_ycf[df_t == 0][:limit], label = \"Control\")\naxs[0].legend(ncol=2)\naxs[1].legend(ncol=2)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-29T13:27:58.836673Z","iopub.execute_input":"2022-04-29T13:27:58.837199Z","iopub.status.idle":"2022-04-29T13:27:59.304974Z","shell.execute_reply.started":"2022-04-29T13:27:58.837166Z","shell.execute_reply":"2022-04-29T13:27:59.304287Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"limit = 150\nfig, axs = plt.subplots(1, 2, figsize=(16, 4))\n# These scatterplots have only been made via factual outcomes \n# More scatterplots could be made to model the counterfactual outcomes\naxs[0].scatter(df_x[:, 1].reshape(-1, 1)[df_t == 1][:limit]\n               , df_yf[df_t == 1][:limit], label = \"Treated\")\naxs[0].scatter(df_x[:, 1].reshape(-1, 1)[df_t == 0][:limit]\n               , df_yf[df_t == 0][:limit], label = \"Control\")\naxs[1].scatter(df_x[:, 1].reshape(-1, 1)[df_t == 1][:limit]\n               , df_ycf[df_t == 1][:limit], label = \"Treated\")\naxs[1].scatter(df_x[:, 1].reshape(-1, 1)[df_t == 0][:limit]\n               , df_ycf[df_t == 0][:limit], label = \"Control\")\naxs[0].legend(ncol=2)\naxs[1].legend(ncol=2)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-29T13:27:59.306156Z","iopub.execute_input":"2022-04-29T13:27:59.306832Z","iopub.status.idle":"2022-04-29T13:27:59.758308Z","shell.execute_reply.started":"2022-04-29T13:27:59.306792Z","shell.execute_reply":"2022-04-29T13:27:59.757760Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(18, 10))\nheatmap = sns.heatmap(IHDP_x.corr(), vmin=-1, vmax=1, annot=True)\nheatmap.set_title('Correlation Heatmap', fontdict={'fontsize':12}, pad=12);","metadata":{"execution":{"iopub.status.busy":"2022-04-29T13:27:59.759685Z","iopub.execute_input":"2022-04-29T13:27:59.760136Z","iopub.status.idle":"2022-04-29T13:28:02.958279Z","shell.execute_reply.started":"2022-04-29T13:27:59.760087Z","shell.execute_reply":"2022-04-29T13:28:02.957552Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"bins=20\nplt.figsize=(16, 4)\nplt.hist(df_t, bins=bins, color = \"orange\")\nplt.title(\"IHDP Control and treatment Distribution\", fontsize=12, fontweight=\"bold\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-29T13:28:02.959569Z","iopub.execute_input":"2022-04-29T13:28:02.960319Z","iopub.status.idle":"2022-04-29T13:28:03.208265Z","shell.execute_reply.started":"2022-04-29T13:28:02.960277Z","shell.execute_reply":"2022-04-29T13:28:03.207448Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"The above graph confirms our need to use X-learner. There is clear inbalance towards the treatment and control groups in both datasets, hopefully X-learner should be able to deal with this when calculating our CATE value. ","metadata":{}},{"cell_type":"markdown","source":"### Standardizing and spliting\n","metadata":{}},{"cell_type":"code","source":"x_train, x_test, t_train, t_test, yf_train, yf_test, ite_train, ite_test = train_test_split(df_x, df_t, df_yf, df_ite, test_size=0.2)","metadata":{"execution":{"iopub.status.busy":"2022-04-29T13:28:03.209463Z","iopub.execute_input":"2022-04-29T13:28:03.209791Z","iopub.status.idle":"2022-04-29T13:28:03.215890Z","shell.execute_reply.started":"2022-04-29T13:28:03.209755Z","shell.execute_reply":"2022-04-29T13:28:03.214827Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"### Random Forest Regressor ","metadata":{}},{"cell_type":"code","source":"# Simple Random forest trained using the IHDP features + treatment\nrandom_forest_IHDP = RandomForestRegressor() \nConcatenated_XT_train_IHDP = np.concatenate([x_train, t_train], axis=1)\nrandom_forest_IHDP.fit(Concatenated_XT_train_IHDP, yf_train.flatten())\n# t=0 - IHDP\nConcatenated_XT_zeros_IHDP = np.concatenate([x_test, np.zeros_like(t_test)], axis=1)\nY0_IHDP = random_forest_IHDP.predict(Concatenated_XT_zeros_IHDP)\n# t=1 - IHDP\nConcatenated_XT_ones_IHDP = np.concatenate([x_test, np.ones_like(t_test)], axis=1)\nY1_IHDP = random_forest_IHDP.predict(Concatenated_XT_ones_IHDP)\n#ITEs for IHDP dataset\n# Effect_Estimates_IHDP\nestimted_eff = Y1_IHDP - Y0_IHDP\n#Metrics\nATE_IHDP = metrics.abs_ate(ite_test, estimted_eff)\nPEHE_IHDP = metrics.pehe(ite_test, estimted_eff)\nprint('ATE, PEHE',ATE_IHDP,PEHE_IHDP )","metadata":{"execution":{"iopub.status.busy":"2022-04-29T13:28:03.217312Z","iopub.execute_input":"2022-04-29T13:28:03.217791Z","iopub.status.idle":"2022-04-29T13:28:03.653408Z","shell.execute_reply.started":"2022-04-29T13:28:03.217745Z","shell.execute_reply":"2022-04-29T13:28:03.652546Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"results = []\nresults.append(['RF', ATE_IHDP, PEHE_IHDP])\ncols = ['Method', 'ATE test', 'PEHE test']\ndf_First = pd.DataFrame(results, columns=cols)\ndf_First","metadata":{"execution":{"iopub.status.busy":"2022-04-29T13:28:03.654818Z","iopub.execute_input":"2022-04-29T13:28:03.655190Z","iopub.status.idle":"2022-04-29T13:28:03.668256Z","shell.execute_reply.started":"2022-04-29T13:28:03.655144Z","shell.execute_reply":"2022-04-29T13:28:03.667210Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":"### XGB Regressor \n\nHere we will start with a simple model similar to the random forest machine learning process, however we will need to standardize the data. Any features that are non binary will be standardized, including the label (assuming it's non-binary). First, we need to check for non binary columns","metadata":{}},{"cell_type":"code","source":"temp_X_IHDP = pd.DataFrame(x_train)\ntemp_X_test_IHDP = pd.DataFrame(x_test)\n#temp_X_IHDP.head()\ntemp_yf_IHDP = pd.DataFrame(yf_train)\n#temp_yf_IHDP.head()\n#[temp_X_IHDP[cols].unique() for cols in temp_X_IHDP]","metadata":{"execution":{"iopub.status.busy":"2022-04-29T13:28:03.670275Z","iopub.execute_input":"2022-04-29T13:28:03.670669Z","iopub.status.idle":"2022-04-29T13:28:03.679068Z","shell.execute_reply.started":"2022-04-29T13:28:03.670570Z","shell.execute_reply":"2022-04-29T13:28:03.678410Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":"#### Scaling the data","metadata":{}},{"cell_type":"markdown","source":"Columns 0-5 all require conventional scaling, however the remainder are binary and so do not. We also know that our outcome column requires Standard scaling based on previous modelling.","metadata":{}},{"cell_type":"code","source":"# IHDP\n# Scale the first 6 columns of our features (all non binary)\ntemp_X_IHDP.iloc[:, 0:5] = StandardScaler().fit_transform(temp_X_IHDP.iloc[:, 0:5])\ntemp_X_test_IHDP.iloc[:, 0:5] = StandardScaler().fit_transform(temp_X_test_IHDP.iloc[:, 0:5])\n# Scale our outcomes column \nyf_train_Stan = StandardScaler().fit_transform(temp_yf_IHDP) \n#temp_X_IHDP.head()\nx_train_Stan = temp_X_IHDP.to_numpy()\nx_test_Stan = temp_X_test_IHDP.to_numpy()","metadata":{"execution":{"iopub.status.busy":"2022-04-29T13:28:03.679940Z","iopub.execute_input":"2022-04-29T13:28:03.680841Z","iopub.status.idle":"2022-04-29T13:28:03.700454Z","shell.execute_reply.started":"2022-04-29T13:28:03.680805Z","shell.execute_reply":"2022-04-29T13:28:03.699587Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"# Concatenating the treatment and feature columns of the datasets \nConcatenated_XT_train_IHDP = np.concatenate([x_train_Stan, t_train], axis=1)\nLinear_Regressor_IHDP = LinearRegression().fit(Concatenated_XT_train_IHDP, yf_train_Stan.flatten())\n# t=0\nConcatenated_XT_zeros_IHDP = np.concatenate([x_test_Stan, np.zeros_like(t_test)], axis=1)\nY0_IHDP = Linear_Regressor_IHDP.predict(Concatenated_XT_zeros_IHDP)\n# t=1\nConcatenated_XT_ones_IHDP = np.concatenate([x_test_Stan, np.ones_like(t_test)], axis=1)\nY1_IHDP = Linear_Regressor_IHDP.predict(Concatenated_XT_ones_IHDP)\n#ITEs for the dataset\nEffect_Estimates_IHDP_LR = Y1_IHDP - Y0_IHDP\n# Metrics\nATE_IHDP_LR = metrics.abs_ate(ite_test, Effect_Estimates_IHDP_LR)\nPEHE_IHDP_LR = metrics.pehe(ite_test, Effect_Estimates_IHDP_LR)","metadata":{"execution":{"iopub.status.busy":"2022-04-29T13:28:03.701706Z","iopub.execute_input":"2022-04-29T13:28:03.701945Z","iopub.status.idle":"2022-04-29T13:28:03.724651Z","shell.execute_reply.started":"2022-04-29T13:28:03.701916Z","shell.execute_reply":"2022-04-29T13:28:03.723170Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"# Metrics for IHDP dataset\nresults = []\nresults.append(['RF', ATE_IHDP, PEHE_IHDP])\nresults.append(['LR', ATE_IHDP_LR, PEHE_IHDP_LR])\ncols = ['Method', 'ATE test', 'PEHE test']\ndf_First = pd.DataFrame(results, columns=cols)\ndf_First","metadata":{"execution":{"iopub.status.busy":"2022-04-29T13:28:03.726210Z","iopub.execute_input":"2022-04-29T13:28:03.726721Z","iopub.status.idle":"2022-04-29T13:28:03.745155Z","shell.execute_reply.started":"2022-04-29T13:28:03.726676Z","shell.execute_reply":"2022-04-29T13:28:03.744291Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":" As it can be seen,The linear regressor performs considerably worse than the Random forest model.","metadata":{}},{"cell_type":"markdown","source":"### Optimized Random Forest Regressor","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\nfrom sklearn.linear_model import LinearRegression\n\nclass Tuning:    \n    Model = RandomForestRegressor()\n    random_grid = {'bootstrap': [True, False],\n         'max_depth': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, None],\n         'max_features': ['auto', 'sqrt'],\n         'min_samples_leaf': [1, 2, 4],\n         'min_samples_split': [2, 5, 10],\n         'n_estimators': [200, 400, 600, 800, 1000, 1200, 1400, 1600, 1800, 2000]}\n    param_grid = {'fit_intercept': [True, False],'positive': [True, False]}\n\n\n    def random_forest(self,XTtrain, Ytrain, Weights=\"None\", Model=Model, random_grid=random_grid):\n            Optimizer = RandomizedSearchCV(estimator = self.Model, param_distributions = self.random_grid, n_iter = 100, cv = 10, verbose=2, n_jobs = -1)\n            if Weights == \"None\":\n                    Optimizer.fit(XTtrain, Ytrain.flatten())\n            else:\n                    Optimizer.fit(XTtrain, Ytrain.flatten(), sample_weight=Weights)\n            return Optimizer.best_params_       \n                \n                ","metadata":{"execution":{"iopub.status.busy":"2022-04-29T13:28:03.746741Z","iopub.execute_input":"2022-04-29T13:28:03.747205Z","iopub.status.idle":"2022-04-29T13:28:03.762741Z","shell.execute_reply.started":"2022-04-29T13:28:03.747164Z","shell.execute_reply":"2022-04-29T13:28:03.761748Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"markdown","source":"#### Grid Search","metadata":{}},{"cell_type":"code","source":"Benchmark_IHDP = RandomForestRegressor()\n# Our parameter Grid uses values centered around the parameters found from the random search\nparam_grid = {\n    'bootstrap': [True],\n    'max_depth': [5, 10 , 20],\n    'max_features': ['auto', 'sqrt'],\n    'min_samples_leaf': [2, 3, 4],\n    'min_samples_split': [2, 4, 6],\n    'n_estimators': [200, 400, 600]\n}\n\nGridSearch_RF_IHDP = GridSearchCV(estimator = Benchmark_IHDP, param_grid = param_grid, cv = 10, n_jobs = -1, verbose = 2)\nGridSearch_RF_IHDP.fit(Concatenated_XT_train_IHDP, yf_train.flatten())\nGridSearch_RF_IHDP.best_params_","metadata":{"execution":{"iopub.status.busy":"2022-04-29T13:28:03.764217Z","iopub.execute_input":"2022-04-29T13:28:03.764719Z","iopub.status.idle":"2022-04-29T13:37:31.108419Z","shell.execute_reply.started":"2022-04-29T13:28:03.764677Z","shell.execute_reply":"2022-04-29T13:37:31.107641Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"markdown","source":"#### Metric Evaluation","metadata":{}},{"cell_type":"markdown","source":"Now we can define our optimized random forest model using the results of the grid serach and evaluate their performances using the usual process ","metadata":{}},{"cell_type":"code","source":"Optimized_RF_IHDP = RandomForestRegressor(bootstrap=True, max_depth=5, max_features='auto', min_samples_leaf=3, min_samples_split=4, n_estimators=400)\nOptimized_RF_IHDP.fit(Concatenated_XT_train_IHDP, yf_train.flatten())\n\n# t=0 - IHDP\nConcatenated_XT_zeros_IHDP = np.concatenate([x_test, np.zeros_like(t_test)], axis=1)\nY0_IHDP = Optimized_RF_IHDP.predict(Concatenated_XT_zeros_IHDP)\n# t=1 - IHDP\nConcatenated_XT_ones_IHDP = np.concatenate([x_test, np.ones_like(t_test)], axis=1)\nY1_IHDP = Optimized_RF_IHDP.predict(Concatenated_XT_ones_IHDP)\n\n#ITEs for IHDP dataset\nEffect_Estimated = Y1_IHDP - Y0_IHDP","metadata":{"execution":{"iopub.status.busy":"2022-04-29T13:37:31.109914Z","iopub.execute_input":"2022-04-29T13:37:31.110143Z","iopub.status.idle":"2022-04-29T13:37:32.114458Z","shell.execute_reply.started":"2022-04-29T13:37:31.110114Z","shell.execute_reply":"2022-04-29T13:37:32.113689Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"markdown","source":"Now once again, we can calculate our metric for each of the datasets","metadata":{}},{"cell_type":"code","source":"ATE_IHDP_Optimized = metrics.abs_ate(ite_test, Effect_Estimated)\nPEHE_IHDP_Optimized = metrics.pehe(ite_test, Effect_Estimated)","metadata":{"execution":{"iopub.status.busy":"2022-04-29T13:37:32.115559Z","iopub.execute_input":"2022-04-29T13:37:32.115798Z","iopub.status.idle":"2022-04-29T13:37:32.120826Z","shell.execute_reply.started":"2022-04-29T13:37:32.115769Z","shell.execute_reply":"2022-04-29T13:37:32.120170Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"results = []\nresults.append(['RF', ATE_IHDP, PEHE_IHDP])\nresults.append(['LR', ATE_IHDP_LR, PEHE_IHDP_LR])\nresults.append(['Optimized RF', ATE_IHDP_Optimized, PEHE_IHDP_Optimized])\ncols = ['Method', 'ATE test', 'PEHE test']\ndf_First = pd.DataFrame(results, columns=cols)\ndf_First","metadata":{"execution":{"iopub.status.busy":"2022-04-29T13:37:32.122005Z","iopub.execute_input":"2022-04-29T13:37:32.122711Z","iopub.status.idle":"2022-04-29T13:37:32.140935Z","shell.execute_reply.started":"2022-04-29T13:37:32.122665Z","shell.execute_reply":"2022-04-29T13:37:32.139965Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"markdown","source":"As denoted by the above results, there is a clear improvement in regards to bth the average treatment effect metric and PEHE; it is clear that optimizing each of the models has a sizeable impact on their performance","metadata":{}},{"cell_type":"markdown","source":"### Linear Regressor - Optimized","metadata":{}},{"cell_type":"markdown","source":"Unfortunately there is not alot of hyperparameter tuning to be done in regards to the linear regressor, therefore we will run only a grid search to find the optimal values for what is available to us","metadata":{}},{"cell_type":"markdown","source":"Unfortunately, there is no real improvement in either average treatment effect or PEHE for the optimized linear regression model; we can see there is still a clear poor performance. ","metadata":{}},{"cell_type":"markdown","source":"### Feature Importances ","metadata":{}},{"cell_type":"markdown","source":"Finally we can plot and vizualise the feature importances of the data, from most useful to least useful. As the columns don't have a title, we can recognize feature by their column indices.","metadata":{}},{"cell_type":"code","source":"from matplotlib import pyplot\n\n# Feature Importances for the IHDP dataset using random forest regression\nOptimized_RF_IHDP.fit(Concatenated_XT_train_IHDP, yf_train.flatten())\nfeatures_IHDP_RF = list(Concatenated_XT_train_IHDP)\nimportances_IHDP_RF = Optimized_RF_IHDP.feature_importances_\nstd_IHDP = np.std([tree.feature_importances_ for tree in Optimized_RF_IHDP.estimators_], axis=0)\nindices_IHDP_RF = np.argsort(importances_IHDP_RF)[::-1]\nprint(\"Top 5 Feature Importances - Random Forest\")\nfor f in range(5):\n    print(\"%d. %s (%f)\" % (f + 1, \"Feature Column: \"+ str(indices_IHDP_RF[f]), importances_IHDP_RF[indices_IHDP_RF[f]]))\npyplot.bar([x for x in range(len(importances_IHDP_RF))], importances_IHDP_RF)\npyplot.title(\"Feature Importance for Jobs dataset using RF\")\npyplot.ylabel(\"Feature Score\")\npyplot.xlabel(\"Feature Number\")\npyplot.show()\n    \n    \n# # Feature Importances for the IHDP dataset using Linear regression\n# LR_IHDP_Optimized.fit(Concatenated_XT_train_IHDP, IHDP_yf_train_Standardized.flatten())\n# importances_IHDP_LR = LR_IHDP_Optimized.coef_\n# indices_IHDP_LR = np.argsort(importances_IHDP_LR)[::-1]\n# print(\"Top 5 Feature Importances - Linear Regressor\")\n# for f in range(5):\n#     print(\"%d. %s (%f)\" % (f + 1, \"Feature Column: \"+ str(indices_IHDP_LR[f]), importances_IHDP_LR[indices_IHDP_LR[f]]))\n    ","metadata":{"execution":{"iopub.status.busy":"2022-04-29T13:37:32.142546Z","iopub.execute_input":"2022-04-29T13:37:32.142862Z","iopub.status.idle":"2022-04-29T13:37:33.378052Z","shell.execute_reply.started":"2022-04-29T13:37:32.142821Z","shell.execute_reply":"2022-04-29T13:37:33.377252Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can clearly see that the most important feature in our IHDP dataset is the feature at column index 25 (We have used column indexes as we don't have titles to refer to). Column 25 in this case is our concatenated treatment data. The feature importances for this dataset are extremely skewed. We can also see that the reliance on the treatment feature is even more apparent in linear regressors then it is for random forest. There doesn't seem to be any obvious feature importance patterns outside of treatment except for the fact that colum index 14 and 5 of the background features consistently feature in the top 5 most important. ","metadata":{}},{"cell_type":"markdown","source":"## Training Our Models - Propensity Score Re-weighting","metadata":{}},{"cell_type":"markdown","source":"For this section we are going to use a random forest classifier. First we need to train a classifer to predict propensity scores based on background features. We also need a function that calculates sample weights based on these propensity scores.","metadata":{}},{"cell_type":"markdown","source":"Let's train our classifier to predic propensity scores and then define each of the variables of the IPSW equation: ti, e(x) Note: e(x) can also be referred to as P(ti|xi)","metadata":{}},{"cell_type":"code","source":"Simple_Classifier_IHDP = RandomForestClassifier()\n\n# These will act as our ti values \nti_IHDP = np.squeeze(t_train)\n\n# Classifier trained to predict propensity scores\nSimple_Classifier_IHDP.fit(x_train, ti_IHDP)\n\n# These will act as our ptx values\nex_IHDP = Simple_Classifier_IHDP.predict_proba(x_train).T[1].T + 0.0001","metadata":{"execution":{"iopub.status.busy":"2022-04-29T13:37:33.383259Z","iopub.execute_input":"2022-04-29T13:37:33.383501Z","iopub.status.idle":"2022-04-29T13:37:33.627367Z","shell.execute_reply.started":"2022-04-29T13:37:33.383472Z","shell.execute_reply":"2022-04-29T13:37:33.626534Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"markdown","source":"Given the equation: \n    \n$$w_i = \\frac{t_i}{e(x_i)} + \\frac{1-t_i}{1-e(x_i)}$$\n\n\nWe can use our imported calc_weights function to calculate our sample weights ","metadata":{}},{"cell_type":"code","source":"import math\ndef Calc_Weights(ti, ex):\n    return (ti / ex) + ((1-ti) / (1-ex))","metadata":{"execution":{"iopub.status.busy":"2022-04-29T13:37:33.628910Z","iopub.execute_input":"2022-04-29T13:37:33.629514Z","iopub.status.idle":"2022-04-29T13:37:33.634945Z","shell.execute_reply.started":"2022-04-29T13:37:33.629469Z","shell.execute_reply":"2022-04-29T13:37:33.633907Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"Sample_Weights_IHDP = Calc_Weights(ti_IHDP, ex_IHDP)","metadata":{"execution":{"iopub.status.busy":"2022-04-29T13:37:33.636353Z","iopub.execute_input":"2022-04-29T13:37:33.636901Z","iopub.status.idle":"2022-04-29T13:37:33.646081Z","shell.execute_reply.started":"2022-04-29T13:37:33.636865Z","shell.execute_reply":"2022-04-29T13:37:33.645496Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"markdown","source":"Now that we have our sample weights, we can train our regressors using the weights","metadata":{}},{"cell_type":"code","source":"# Training variables for random forest \nConcatenated_XT_train_IHDP = np.concatenate([x_train, t_train], axis=1)\n# Training variables for linear regression \n# Concatenated_XT_train_IHDP_Standardized = np.concatenate([IHDP_x_train_Standardized, IHDP_t_train], axis=1)\n\nRandomForest_IHDP_IPSW = RandomForestRegressor()\n# LinearRegressor_IHDP_IPSW = LinearRegression()\n\n# Trained regressors\nRandomForest_IHDP_IPSW.fit(Concatenated_XT_train_IHDP, yf_train.flatten(), sample_weight=Sample_Weights_IHDP)\n# LinearRegressor_IHDP_IPSW.fit(Concatenated_XT_train_IHDP_Standardized, IHDP_yf_train_Standardized.flatten(), sample_weight=Sample_Weights_IHDP)","metadata":{"execution":{"iopub.status.busy":"2022-04-29T13:37:33.647006Z","iopub.execute_input":"2022-04-29T13:37:33.647648Z","iopub.status.idle":"2022-04-29T13:37:34.067571Z","shell.execute_reply.started":"2022-04-29T13:37:33.647611Z","shell.execute_reply":"2022-04-29T13:37:34.066941Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"markdown","source":"Now we can repeat previous steps for both models for hyperparameter optimization and metric evaluation, this starts with a Random and Grid search for the Random forest model.","metadata":{}},{"cell_type":"markdown","source":"#### Random Search","metadata":{}},{"cell_type":"code","source":"# tuning = Tuning()\n# print(tuning.random_forest(Concatenated_XT_train_IHDP, yf_train, Weights=Sample_Weights_IHDP, Model=RandomForest_IHDP_IPSW))","metadata":{"execution":{"iopub.status.busy":"2022-04-29T13:37:34.068655Z","iopub.execute_input":"2022-04-29T13:37:34.069218Z","iopub.status.idle":"2022-04-29T13:37:34.072213Z","shell.execute_reply.started":"2022-04-29T13:37:34.069185Z","shell.execute_reply":"2022-04-29T13:37:34.071384Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"markdown","source":"#### Grid Search - Both Random forest and Linear Regressor","metadata":{}},{"cell_type":"code","source":"# Our parameter Grid uses values centered around the parameters found from the random search\nparam_grid = {\n    'bootstrap': [True],\n    'max_depth': [8, 10, 12],\n    'max_features': ['auto', 'sqrt'],\n    'min_samples_leaf': [2,3,4],\n    'min_samples_split': [4, 5, 6],\n    'n_estimators': [1000, 1200, 1400]\n}\n\nGridSearch_RF_IHDP_IPSW = GridSearchCV(estimator = RandomForest_IHDP_IPSW, param_grid = param_grid, cv = 10, n_jobs = -1, verbose = 2)\nGridSearch_RF_IHDP_IPSW.fit(Concatenated_XT_train_IHDP, yf_train.flatten(), sample_weight=Sample_Weights_IHDP)\nGridSearch_RF_IHDP_IPSW.best_params_","metadata":{"execution":{"iopub.status.busy":"2022-04-29T13:37:34.073218Z","iopub.execute_input":"2022-04-29T13:37:34.073434Z","iopub.status.idle":"2022-04-29T14:06:34.187121Z","shell.execute_reply.started":"2022-04-29T13:37:34.073401Z","shell.execute_reply":"2022-04-29T14:06:34.186209Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"# print(Optimize_LR(Concatenated_XT_train_IHDP_Standardized, IHDP_yf_train_Standardized,Weights=Sample_Weights_IHDP, ModelTwo=LinearRegressor_IHDP_IPSW))","metadata":{"execution":{"iopub.status.busy":"2022-04-29T14:06:34.188517Z","iopub.execute_input":"2022-04-29T14:06:34.188835Z","iopub.status.idle":"2022-04-29T14:06:34.193488Z","shell.execute_reply.started":"2022-04-29T14:06:34.188792Z","shell.execute_reply":"2022-04-29T14:06:34.192671Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"markdown","source":"#### Metric Evaluation","metadata":{}},{"cell_type":"markdown","source":"As usual, we begin by establishing our optimal IPSW models, then calculate our predictions for treatment = 1 and treatment = 0, and then estimate treatment effects.","metadata":{}},{"cell_type":"code","source":"RF_IPSW_IHDP = RandomForestRegressor(bootstrap=True, max_depth=8, max_features='auto', min_samples_leaf=2, min_samples_split=5, n_estimators=1000)\nRF_IPSW_IHDP.fit(Concatenated_XT_train_IHDP, yf_train.flatten(), sample_weight=Sample_Weights_IHDP)","metadata":{"execution":{"iopub.status.busy":"2022-04-29T14:06:34.195021Z","iopub.execute_input":"2022-04-29T14:06:34.195319Z","iopub.status.idle":"2022-04-29T14:06:36.979577Z","shell.execute_reply.started":"2022-04-29T14:06:34.195279Z","shell.execute_reply":"2022-04-29T14:06:36.978721Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"# t=0 - IHDP RF\nConcatenated_XT_zeros_IHDP = np.concatenate([x_test, np.zeros_like(t_test)], axis=1)\nY0_IHDP_IPSW_RF = RF_IPSW_IHDP.predict(Concatenated_XT_zeros_IHDP)\n# t=1 - IHDP RF\nConcatenated_XT_ones_IHDP = np.concatenate([x_test, np.ones_like(t_test)], axis=1)\nY1_IHDP_IPSW_RF = RF_IPSW_IHDP.predict(Concatenated_XT_ones_IHDP)\n\n# # t=0 - IHDP LR\n# Concatenated_XT_zeros_IHDP = np.concatenate([x_test_Stan, np.zeros_like(t_test)], axis=1)\n# Y0_IHDP_IPSW_LR = LR_IPSW_IHDP.predict(Concatenated_XT_zeros_IHDP)\n# # t=1 - IHDP LR\n# Concatenated_XT_ones_IHDP = np.concatenate([x_test_Stan, np.ones_like(t_test)], axis=1)\n# Y1_IHDP_IPSW_LR = LR_IPSW_IHDP.predict(Concatenated_XT_ones_IHDP)","metadata":{"execution":{"iopub.status.busy":"2022-04-29T14:06:36.981202Z","iopub.execute_input":"2022-04-29T14:06:36.981512Z","iopub.status.idle":"2022-04-29T14:06:37.187111Z","shell.execute_reply.started":"2022-04-29T14:06:36.981471Z","shell.execute_reply":"2022-04-29T14:06:37.186235Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"#ITEs for Random Forest Model IHDP IPSW\nestimated_effect_RF_IPSW = Y1_IHDP_IPSW_RF - Y0_IHDP_IPSW_RF\n# #ITEs for Linear Regression Model IHDP IPSW\n# Effect_Estimates_IHDP_LR_IPSW = Y1_IHDP_IPSW_LR - Y0_IHDP_IPSW_LR","metadata":{"execution":{"iopub.status.busy":"2022-04-29T14:06:37.188607Z","iopub.execute_input":"2022-04-29T14:06:37.188909Z","iopub.status.idle":"2022-04-29T14:06:37.193435Z","shell.execute_reply.started":"2022-04-29T14:06:37.188870Z","shell.execute_reply":"2022-04-29T14:06:37.192587Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"# Metric calculations for random forest IPSW\n# metrics = Metrics()\nATE_IHDP_RF_IPSW = metrics.abs_ate(ite_test, estimated_effect_RF_IPSW)\nPEHE_IHDP_RF_IPSW = metrics.pehe(ite_test, estimated_effect_RF_IPSW)\n# Metric calculations for linear regression IPSW\nATE_IHDP_LR_IPSW = metrics.abs_ate(ite_test, estimated_effect_RF_IPSW)\nPEHE_IHDP_LR_IPSW = metrics.pehe(ite_test, estimated_effect_RF_IPSW)","metadata":{"execution":{"iopub.status.busy":"2022-04-29T14:06:37.195006Z","iopub.execute_input":"2022-04-29T14:06:37.195304Z","iopub.status.idle":"2022-04-29T14:06:37.209632Z","shell.execute_reply.started":"2022-04-29T14:06:37.195265Z","shell.execute_reply":"2022-04-29T14:06:37.208658Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"results = []\nresults.append(['RF', ATE_IHDP, PEHE_IHDP])\nresults.append(['LR', ATE_IHDP_LR, PEHE_IHDP_LR])\nresults.append(['Optimized RF', ATE_IHDP_Optimized, PEHE_IHDP_Optimized])\n# results.append(['Optimized LR', ATE_IHDP_LR_Optimized, PEHE_IHDP_LR_Optimized])\nresults.append(['IPSW RF', ATE_IHDP_RF_IPSW, PEHE_IHDP_RF_IPSW])\n# results.append(['IPSW LR',ATE_IHDP_LR_IPSW,PEHE_IHDP_LR_IPSW])\ncols = ['Method', 'ATE test', 'PEHE test']\ndf_First = pd.DataFrame(results, columns=cols)\ndf_First","metadata":{"execution":{"iopub.status.busy":"2022-04-29T14:06:37.210471Z","iopub.execute_input":"2022-04-29T14:06:37.210745Z","iopub.status.idle":"2022-04-29T14:06:37.228497Z","shell.execute_reply.started":"2022-04-29T14:06:37.210716Z","shell.execute_reply":"2022-04-29T14:06:37.227696Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"markdown","source":"The inverse propensity score re-weighted models perform similarly to the others, outperforming the standard but not the optimised models. The optimised models appear to be optimal thus far. However, we must now examine how the relative relevance of the features has changed.","metadata":{}},{"cell_type":"markdown","source":"### Feature Importances for IPSW ","metadata":{}},{"cell_type":"code","source":"# Feature Importances for the IHDP dataset using random forest regression\nimportances_IHDP_RF = RF_IPSW_IHDP.feature_importances_\nstd_IHDP = np.std([tree.feature_importances_ for tree in RF_IPSW_IHDP.estimators_], axis=0)\nindices_IHDP_RF = np.argsort(importances_IHDP_RF)[::-1]\nprint(\"Feature Importances - RF IPSW\")\nfor f in range(5):\n    print(\"%d. %s (%f)\" % (f + 1, \"Feature Column: \"+ str(indices_IHDP_RF[f]), importances_IHDP_RF[indices_IHDP_RF[f]]))\npyplot.bar([x for x in range(len(importances_IHDP_RF))], importances_IHDP_RF)\npyplot.title(\"Feature Importance for Jobs dataset using RF\")\npyplot.ylabel(\"Feature Score\")\npyplot.xlabel(\"Feature Number\")\npyplot.show()\n# # Feature Importances for the IHDP dataset using linear regression\n# importances_IHDP_LR = LR_IPSW_IHDP.coef_\n# indices_IHDP_LR = np.argsort(importances_IHDP_LR)[::-1]\n# print(\"Feature Importances - LR IPSW\")\n# for f in range(5):\n#     print(\"%d. %s (%f)\" % (f + 1, \"Feature Column: \"+ str(indices_IHDP_LR[f]), importances_IHDP_LR[indices_IHDP_LR[f]]))","metadata":{"execution":{"iopub.status.busy":"2022-04-29T14:06:37.229780Z","iopub.execute_input":"2022-04-29T14:06:37.230482Z","iopub.status.idle":"2022-04-29T14:06:37.597978Z","shell.execute_reply.started":"2022-04-29T14:06:37.230441Z","shell.execute_reply":"2022-04-29T14:06:37.597328Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"markdown","source":"Interestingly, the importance of treatment has not increased. Weighting usually minimises distributional differences between the treated and control units, thus I'd expect t's importance to rise. As can be seen from the graphs above, the non-weighted model has already determined the value of the treatment feature and hence may not require the increase.","metadata":{}},{"cell_type":"markdown","source":"## Advanced CATE Estimators","metadata":{}},{"cell_type":"markdown","source":"RF as the base learner of XLearner","metadata":{}},{"cell_type":"code","source":"xl_IHDP = XLearner(models=RandomForestRegressor(), propensity_model=RandomForestClassifier())\nxl_IHDP.fit(yf_train, t_train.flatten(), X=x_train)\nxl_te_test_IHDP = xl_IHDP.effect(x_test)","metadata":{"execution":{"iopub.status.busy":"2022-04-29T14:06:37.605569Z","iopub.execute_input":"2022-04-29T14:06:37.605872Z","iopub.status.idle":"2022-04-29T14:06:38.957932Z","shell.execute_reply.started":"2022-04-29T14:06:37.605829Z","shell.execute_reply":"2022-04-29T14:06:38.956896Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"markdown","source":"ATE and PEHE metrics ","metadata":{}},{"cell_type":"code","source":"xl_ate_test = metrics.abs_ate(ite_test, xl_te_test_IHDP)\nxl_pehe_test = metrics.pehe(ite_test, xl_te_test_IHDP)","metadata":{"execution":{"iopub.status.busy":"2022-04-29T14:13:49.751807Z","iopub.execute_input":"2022-04-29T14:13:49.752126Z","iopub.status.idle":"2022-04-29T14:13:49.756531Z","shell.execute_reply.started":"2022-04-29T14:13:49.752089Z","shell.execute_reply":"2022-04-29T14:13:49.755831Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"markdown","source":"Now we can observe the performance of all the models in this script, all in the same table ","metadata":{}},{"cell_type":"code","source":"results = []\nresults.append(['Linear Regression', ATE_IHDP_LR, PEHE_IHDP_LR])\nresults.append(['Random Forest', ATE_IHDP, PEHE_IHDP])\nresults.append(['Hyperparameter tuned Random Forest', ATE_IHDP_Optimized, PEHE_IHDP_Optimized])\n# results.append(['Optimized LR', ATE_IHDP_LR_Optimized, PEHE_IHDP_LR_Optimized])\nresults.append(['IPSW Random Forest', ATE_IHDP_RF_IPSW, PEHE_IHDP_RF_IPSW])\n# results.append(['IPSW LR',ATE_IHDP_LR_IPSW,PEHE_IHDP_LR_IPSW])\nresults.append(['X-Learner',xl_ate_test ,xl_pehe_test])\ncols = ['Methodology', 'ATE test', 'PEHE test']\ndf_First = pd.DataFrame(results, columns=cols)\ndf_First","metadata":{"execution":{"iopub.status.busy":"2022-04-29T14:13:51.796501Z","iopub.execute_input":"2022-04-29T14:13:51.796785Z","iopub.status.idle":"2022-04-29T14:13:51.811920Z","shell.execute_reply.started":"2022-04-29T14:13:51.796754Z","shell.execute_reply":"2022-04-29T14:13:51.810802Z"},"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"markdown","source":"Looking at the above table it is clear that XLearner is by far the most consistent model, with the best ATE result and the second best PEHE result.","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}