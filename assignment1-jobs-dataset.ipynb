{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install econml","metadata":{"execution":{"iopub.status.busy":"2022-05-03T12:11:04.417556Z","iopub.execute_input":"2022-05-03T12:11:04.418456Z","iopub.status.idle":"2022-05-03T12:11:22.687392Z","shell.execute_reply.started":"2022-05-03T12:11:04.418320Z","shell.execute_reply":"2022-05-03T12:11:22.686473Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Required Libraries\nfrom econml.metalearners import XLearner\nfrom sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.model_selection import GridSearchCV\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport scipy.stats as st\nimport matplotlib.pyplot as plt","metadata":{"execution":{"iopub.status.busy":"2022-05-03T12:11:22.689593Z","iopub.execute_input":"2022-05-03T12:11:22.689860Z","iopub.status.idle":"2022-05-03T12:11:32.585533Z","shell.execute_reply.started":"2022-05-03T12:11:22.689821Z","shell.execute_reply":"2022-05-03T12:11:32.584830Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class metrics:\n    \n    def pehe(self,effect_true, effect_pred):\n        \"\"\"\n        Precision in Estimating the Heterogeneous Treatment Effect (PEHE)\n        :param effect_true: true treatment effect value\n        :param effect_pred: predicted treatment effect value\n        :return: PEHE\n        \"\"\"\n        return np.abs(np.mean(effect_pred) - np.mean(effect_true))\n\n    def abs_ate(self,effect_true, effect_pred):\n        \"\"\"\n        Absolute error for the Average Treatment Effect (ATE)\n        :param effect_true: true treatment effect value\n        :param effect_pred: predicted treatment effect value\n        :return: absolute error on ATE\n        \"\"\"\n        return np.sqrt(np.mean((effect_true - effect_pred)**2))\n    @staticmethod\n    def abs_att(effect_pred, yf, t, e):\n        \"\"\"\n        Absolute error for the Average Treatment Effect on the Treated\n        :param effect_pred: predicted treatment effect value\n        :param yf: factual (observed) outcome\n        :param t: treatment status (treated/control)\n        :param e: whether belongs to the experimental group\n        :return: absolute error on ATT\n        \"\"\"\n        att_true = np.mean(yf[t > 0]) - np.mean(yf[(1 - t + e) > 1])\n        att_pred = np.mean(effect_pred[(t + e) > 1])\n\n        return np.abs(att_pred - att_true)\n    @staticmethod\n    def policy_risk(effect_pred, yf, t, e):\n        \"\"\"\n        Computes the risk of the policy defined by predicted effect\n        :param effect_pred: predicted treatment effect value\n        :param yf: factual (observed) outcome\n        :param t: treatment status (treated/control)\n        :param e: whether belongs to the experimental group\n        :return: policy risk\n        \"\"\"\n        # Consider only the cases for which we have experimental data (i.e., e > 0)\n        t_e = t[e > 0]\n        yf_e = yf[e > 0]\n        effect_pred_e = effect_pred[e > 0]\n\n        if np.any(np.isnan(effect_pred_e)):\n            return np.nan\n\n        policy = effect_pred_e > 0.0\n        treat_overlap = (policy == t_e) * (t_e > 0)\n        control_overlap = (policy == t_e) * (t_e < 1)\n\n        if np.sum(treat_overlap) == 0:\n            treat_value = 0\n        else:\n            treat_value = np.mean(yf_e[treat_overlap])\n\n        if np.sum(control_overlap) == 0:\n            control_value = 0\n        else:\n            control_value = np.mean(yf_e[control_overlap])\n\n        pit = np.mean(policy)\n        policy_value = pit * treat_value + (1.0 - pit) * control_value\n\n        return 1.0 - policy_value\n","metadata":{"execution":{"iopub.status.busy":"2022-05-03T12:11:32.587064Z","iopub.execute_input":"2022-05-03T12:11:32.587302Z","iopub.status.idle":"2022-05-03T12:11:32.599863Z","shell.execute_reply.started":"2022-05-03T12:11:32.587268Z","shell.execute_reply":"2022-05-03T12:11:32.598548Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"metrics = metrics()\n","metadata":{"execution":{"iopub.status.busy":"2022-05-03T12:11:32.602192Z","iopub.execute_input":"2022-05-03T12:11:32.602612Z","iopub.status.idle":"2022-05-03T12:11:32.616147Z","shell.execute_reply.started":"2022-05-03T12:11:32.602577Z","shell.execute_reply":"2022-05-03T12:11:32.615460Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Data exploration, Preprocessing & Modelling","metadata":{}},{"cell_type":"code","source":"# loading jobs Dataset\ndf = np.load('../input/datatest/jobs.npz')\n\"\"\"x = Feature Variable, t --> Treatment, y --> Outcome Variable (Factual)\n   e --> experimental or observational data\"\"\"\nfor f in df.files:\n  print(f'{f}: {df[f].shape}')\njx , jt , jy, je = df['x'], df['t'], df['y'], df['e']\ndfX,dfT,dfY,dfE =  pd.DataFrame(df['x']),pd.DataFrame(df['t']),pd.DataFrame(df['y']),pd.DataFrame(df['e'])\nprint(dfX.info())\n","metadata":{"execution":{"iopub.status.busy":"2022-05-03T12:11:32.617594Z","iopub.execute_input":"2022-05-03T12:11:32.617845Z","iopub.status.idle":"2022-05-03T12:11:32.671852Z","shell.execute_reply.started":"2022-05-03T12:11:32.617812Z","shell.execute_reply":"2022-05-03T12:11:32.671047Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dfX.describe().T","metadata":{"execution":{"iopub.status.busy":"2022-05-03T12:11:32.673121Z","iopub.execute_input":"2022-05-03T12:11:32.673366Z","iopub.status.idle":"2022-05-03T12:11:32.735692Z","shell.execute_reply.started":"2022-05-03T12:11:32.673333Z","shell.execute_reply":"2022-05-03T12:11:32.734873Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dfX.boxplot()","metadata":{"execution":{"iopub.status.busy":"2022-05-03T12:11:32.737080Z","iopub.execute_input":"2022-05-03T12:11:32.737356Z","iopub.status.idle":"2022-05-03T12:11:33.186519Z","shell.execute_reply.started":"2022-05-03T12:11:32.737318Z","shell.execute_reply":"2022-05-03T12:11:33.185866Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.pairplot(data=dfX)","metadata":{"execution":{"iopub.status.busy":"2022-05-03T12:11:33.187754Z","iopub.execute_input":"2022-05-03T12:11:33.188034Z","iopub.status.idle":"2022-05-03T12:12:31.888738Z","shell.execute_reply.started":"2022-05-03T12:11:33.187982Z","shell.execute_reply":"2022-05-03T12:12:31.887972Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nfig, axs = plt.subplots(1,4, figsize=(16, 4))\naxs[0].hist(dfX, bins=20)\naxs[1].hist(dfT, bins=20)\naxs[2].hist(jy, bins=20)\naxs[3].hist(je, bins=20)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-03T12:12:31.889809Z","iopub.execute_input":"2022-05-03T12:12:31.890044Z","iopub.status.idle":"2022-05-03T12:12:33.047285Z","shell.execute_reply.started":"2022-05-03T12:12:31.889998Z","shell.execute_reply":"2022-05-03T12:12:33.045522Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(16, 10))\nheatmap = sns.heatmap(dfX.corr(), vmin=-1, vmax=1, annot=True)\nheatmap.set_title('Correlation Heatmap', fontdict={'fontsize':12}, pad=12);","metadata":{"execution":{"iopub.status.busy":"2022-05-03T12:12:33.052827Z","iopub.execute_input":"2022-05-03T12:12:33.054955Z","iopub.status.idle":"2022-05-03T12:12:34.520509Z","shell.execute_reply.started":"2022-05-03T12:12:33.054914Z","shell.execute_reply":"2022-05-03T12:12:34.519850Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Because there appear to be no missing data or non-numerical values in jobs, there is no need for preprocessing when encoding and filling Nan rows, as there is with the IHDP dataset. Jobs, like IHDP, have a lot of outliers in the background variables, which requires a similar experiment with normalisation approach. However, we may investigate random forest regression models, which should manage any outliers internally and reduce the likelihood of our results being skewed or biassed in any way.","metadata":{}},{"cell_type":"code","source":"dfX.hist(bins=25,figsize=(14,10))\n","metadata":{"execution":{"iopub.status.busy":"2022-05-03T12:12:34.521744Z","iopub.execute_input":"2022-05-03T12:12:34.524178Z","iopub.status.idle":"2022-05-03T12:12:37.000528Z","shell.execute_reply.started":"2022-05-03T12:12:34.524135Z","shell.execute_reply":"2022-05-03T12:12:36.999851Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"the background variables in Jobs seem to be unbalanced","metadata":{}},{"cell_type":"code","source":"fig, axs = plt.subplots(1, 2, figsize=(16, 4))\nlimit = 20\naxs[0].scatter(jx[:, 0].reshape(-1, 1)[jt == 1][:limit]\n               , jy[jt == 1][:limit], label = \"Treated\")\naxs[0].scatter(jx[:, 0].reshape(-1, 1)[jt == 0][:limit]\n               , jy[jt == 0][:limit], label = \"Control\")\naxs[1].scatter(jx[:, 1].reshape(-1, 1)[jt == 1][:limit]\n               , jy[jt == 1][:limit], label = \"Treated\")\naxs[1].scatter(jx[:, 1].reshape(-1, 1)[jt == 0][:limit]\n               , jy[jt == 0][:limit], label = \"Control\")\naxs[0].legend(ncol=2)\naxs[1].legend(ncol=2)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-03T12:12:37.001966Z","iopub.execute_input":"2022-05-03T12:12:37.002435Z","iopub.status.idle":"2022-05-03T12:12:37.356502Z","shell.execute_reply.started":"2022-05-03T12:12:37.002395Z","shell.execute_reply":"2022-05-03T12:12:37.355848Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"In contrast to IHDP, jobsÂ outcomes are recorded as binary variables, therefore scatter points are only plotted on 0 and 1. Given the four graphs above, it's far more difficult to spot noticeable effects; yet, given the background variables we've chosen to depict, this could just be a coincidence.","metadata":{}},{"cell_type":"code","source":"fig, axs = plt.subplots(1, 2, figsize=(16, 4))\nlimit = 20\naxs[0].scatter(jx[:, 2].reshape(-1, 1)[jt == 1][:limit],\n               jy[jt == 1][:limit], label = \"Treated\")\naxs[0].scatter(jx[:, 2].reshape(-1, 1)[jt == 0][:limit]\n               , jy[jt == 0][:limit], label = \"Control\")\naxs[1].scatter(jx[:, 3].reshape(-1, 1)[jt == 1][:limit]\n               , jy[jt == 1][:limit], label = \"Treated\")\naxs[1].scatter(jx[:, 3].reshape(-1, 1)[jt == 0][:limit],\n               jy[jt == 0][:limit], label = \"Control\")\naxs[0].legend(ncol=2)\naxs[1].legend(ncol=2)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-03T12:12:37.357885Z","iopub.execute_input":"2022-05-03T12:12:37.358331Z","iopub.status.idle":"2022-05-03T12:12:37.700467Z","shell.execute_reply.started":"2022-05-03T12:12:37.358294Z","shell.execute_reply":"2022-05-03T12:12:37.699830Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bins=20\nplt.hist(jt, bins=bins, color='hotpink')\nplt.title(\"Treatment and Control Distribution\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-03T12:12:37.701712Z","iopub.execute_input":"2022-05-03T12:12:37.702084Z","iopub.status.idle":"2022-05-03T12:12:37.907191Z","shell.execute_reply.started":"2022-05-03T12:12:37.702049Z","shell.execute_reply":"2022-05-03T12:12:37.906524Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The graphs above demonstrate why we need to apply X-learner. In both datasets, there is an obvious imbalance in favour of the treatment and control groups; hopefully, X-learner will be able to account for this when calculating our CATE value.","metadata":{}},{"cell_type":"markdown","source":"### Data Modelling and Standardizing\n","metadata":{}},{"cell_type":"code","source":"jx_train, jx_test, jt_train, jt_test, jy_train, jy_test, je_train, je_test = train_test_split(jx, jt, jy, je, test_size=0.2)","metadata":{"execution":{"iopub.status.busy":"2022-05-03T12:12:37.908537Z","iopub.execute_input":"2022-05-03T12:12:37.908777Z","iopub.status.idle":"2022-05-03T12:12:37.914835Z","shell.execute_reply.started":"2022-05-03T12:12:37.908743Z","shell.execute_reply":"2022-05-03T12:12:37.914059Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"temp_XJ = pd.DataFrame(jx_train)\ntemp_XJ_t = pd.DataFrame(jx_test)\n#temp_X_Jobs.head()\n#[temp_X_Jobs[cols].unique() for cols in temp_X_Jobs]\ntemp_XJ.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-03T12:12:38.774925Z","iopub.execute_input":"2022-05-03T12:12:38.775182Z","iopub.status.idle":"2022-05-03T12:12:38.798195Z","shell.execute_reply.started":"2022-05-03T12:12:38.775148Z","shell.execute_reply":"2022-05-03T12:12:38.797362Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Jobs\n# Scale columns 0,1,6,7,8,9,10,11,12,15 (all non binary)\ntemp_XJ.iloc[:, [0,1,6,7,8,9,10,11,12,15]] = StandardScaler().fit_transform(temp_XJ.iloc[:, [0,1,6,7,8,9,10,11,12,15]])\ntemp_XJ_t.iloc[:, [0,1,6,7,8,9,10,11,12,15]] = StandardScaler().fit_transform(temp_XJ_t.iloc[:, [0,1,6,7,8,9,10,11,12,15]]) \nJobs_xtrain_stan = temp_XJ.to_numpy()\nJobs_xtest_stan = temp_XJ_t.to_numpy()\ntemp_XJ.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-03T12:12:38.799528Z","iopub.execute_input":"2022-05-03T12:12:38.800527Z","iopub.status.idle":"2022-05-03T12:12:38.837930Z","shell.execute_reply.started":"2022-05-03T12:12:38.800487Z","shell.execute_reply":"2022-05-03T12:12:38.837008Z"},"trusted":true},"execution_count":null,"outputs":[]}]}