{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install econml","metadata":{"execution":{"iopub.status.busy":"2022-05-03T12:11:04.417556Z","iopub.execute_input":"2022-05-03T12:11:04.418456Z","iopub.status.idle":"2022-05-03T12:11:22.687392Z","shell.execute_reply.started":"2022-05-03T12:11:04.418320Z","shell.execute_reply":"2022-05-03T12:11:22.686473Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"#Required Libraries\nfrom econml.metalearners import XLearner\nfrom sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.model_selection import GridSearchCV\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport scipy.stats as st\nimport matplotlib.pyplot as plt","metadata":{"execution":{"iopub.status.busy":"2022-05-03T12:11:22.689593Z","iopub.execute_input":"2022-05-03T12:11:22.689860Z","iopub.status.idle":"2022-05-03T12:11:32.585533Z","shell.execute_reply.started":"2022-05-03T12:11:22.689821Z","shell.execute_reply":"2022-05-03T12:11:32.584830Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"class metrics:\n    \n    def pehe(self,effect_true, effect_pred):\n        \"\"\"\n        Precision in Estimating the Heterogeneous Treatment Effect (PEHE)\n        :param effect_true: true treatment effect value\n        :param effect_pred: predicted treatment effect value\n        :return: PEHE\n        \"\"\"\n        return np.abs(np.mean(effect_pred) - np.mean(effect_true))\n\n    def abs_ate(self,effect_true, effect_pred):\n        \"\"\"\n        Absolute error for the Average Treatment Effect (ATE)\n        :param effect_true: true treatment effect value\n        :param effect_pred: predicted treatment effect value\n        :return: absolute error on ATE\n        \"\"\"\n        return np.sqrt(np.mean((effect_true - effect_pred)**2))\n    @staticmethod\n    def abs_att(effect_pred, yf, t, e):\n        \"\"\"\n        Absolute error for the Average Treatment Effect on the Treated\n        :param effect_pred: predicted treatment effect value\n        :param yf: factual (observed) outcome\n        :param t: treatment status (treated/control)\n        :param e: whether belongs to the experimental group\n        :return: absolute error on ATT\n        \"\"\"\n        att_true = np.mean(yf[t > 0]) - np.mean(yf[(1 - t + e) > 1])\n        att_pred = np.mean(effect_pred[(t + e) > 1])\n\n        return np.abs(att_pred - att_true)\n    @staticmethod\n    def policy_risk(effect_pred, yf, t, e):\n        \"\"\"\n        Computes the risk of the policy defined by predicted effect\n        :param effect_pred: predicted treatment effect value\n        :param yf: factual (observed) outcome\n        :param t: treatment status (treated/control)\n        :param e: whether belongs to the experimental group\n        :return: policy risk\n        \"\"\"\n        # Consider only the cases for which we have experimental data (i.e., e > 0)\n        t_e = t[e > 0]\n        yf_e = yf[e > 0]\n        effect_pred_e = effect_pred[e > 0]\n\n        if np.any(np.isnan(effect_pred_e)):\n            return np.nan\n\n        policy = effect_pred_e > 0.0\n        treat_overlap = (policy == t_e) * (t_e > 0)\n        control_overlap = (policy == t_e) * (t_e < 1)\n\n        if np.sum(treat_overlap) == 0:\n            treat_value = 0\n        else:\n            treat_value = np.mean(yf_e[treat_overlap])\n\n        if np.sum(control_overlap) == 0:\n            control_value = 0\n        else:\n            control_value = np.mean(yf_e[control_overlap])\n\n        pit = np.mean(policy)\n        policy_value = pit * treat_value + (1.0 - pit) * control_value\n\n        return 1.0 - policy_value\n","metadata":{"execution":{"iopub.status.busy":"2022-05-03T12:11:32.587064Z","iopub.execute_input":"2022-05-03T12:11:32.587302Z","iopub.status.idle":"2022-05-03T12:11:32.599863Z","shell.execute_reply.started":"2022-05-03T12:11:32.587268Z","shell.execute_reply":"2022-05-03T12:11:32.598548Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"metrics = metrics()\n","metadata":{"execution":{"iopub.status.busy":"2022-05-03T12:11:32.602192Z","iopub.execute_input":"2022-05-03T12:11:32.602612Z","iopub.status.idle":"2022-05-03T12:11:32.616147Z","shell.execute_reply.started":"2022-05-03T12:11:32.602577Z","shell.execute_reply":"2022-05-03T12:11:32.615460Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"### Data exploration, Preprocessing & Modelling","metadata":{}},{"cell_type":"code","source":"# loading jobs Dataset\ndf = np.load('../input/datatest/jobs.npz')\n\"\"\"x = Feature Variable, t --> Treatment, y --> Outcome Variable (Factual)\n   e --> experimental or observational data\"\"\"\nfor f in df.files:\n  print(f'{f}: {df[f].shape}')\njx , jt , jy, je = df['x'], df['t'], df['y'], df['e']\ndfX,dfT,dfY,dfE =  pd.DataFrame(df['x']),pd.DataFrame(df['t']),pd.DataFrame(df['y']),pd.DataFrame(df['e'])\nprint(dfX.info())\n","metadata":{"execution":{"iopub.status.busy":"2022-05-03T12:11:32.617594Z","iopub.execute_input":"2022-05-03T12:11:32.617845Z","iopub.status.idle":"2022-05-03T12:11:32.671852Z","shell.execute_reply.started":"2022-05-03T12:11:32.617812Z","shell.execute_reply":"2022-05-03T12:11:32.671047Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"dfX.describe().T","metadata":{"execution":{"iopub.status.busy":"2022-05-03T12:11:32.673121Z","iopub.execute_input":"2022-05-03T12:11:32.673366Z","iopub.status.idle":"2022-05-03T12:11:32.735692Z","shell.execute_reply.started":"2022-05-03T12:11:32.673333Z","shell.execute_reply":"2022-05-03T12:11:32.734873Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"dfX.boxplot()","metadata":{"execution":{"iopub.status.busy":"2022-05-03T12:11:32.737080Z","iopub.execute_input":"2022-05-03T12:11:32.737356Z","iopub.status.idle":"2022-05-03T12:11:33.186519Z","shell.execute_reply.started":"2022-05-03T12:11:32.737318Z","shell.execute_reply":"2022-05-03T12:11:33.185866Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"sns.pairplot(data=dfX)","metadata":{"execution":{"iopub.status.busy":"2022-05-03T12:11:33.187754Z","iopub.execute_input":"2022-05-03T12:11:33.188034Z","iopub.status.idle":"2022-05-03T12:12:31.888738Z","shell.execute_reply.started":"2022-05-03T12:11:33.187982Z","shell.execute_reply":"2022-05-03T12:12:31.887972Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"\nfig, axs = plt.subplots(1,4, figsize=(16, 4))\naxs[0].hist(dfX, bins=20)\naxs[1].hist(dfT, bins=20)\naxs[2].hist(jy, bins=20)\naxs[3].hist(je, bins=20)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-03T12:12:31.889809Z","iopub.execute_input":"2022-05-03T12:12:31.890044Z","iopub.status.idle":"2022-05-03T12:12:33.047285Z","shell.execute_reply.started":"2022-05-03T12:12:31.889998Z","shell.execute_reply":"2022-05-03T12:12:33.045522Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(16, 10))\nheatmap = sns.heatmap(dfX.corr(), vmin=-1, vmax=1, annot=True)\nheatmap.set_title('Correlation Heatmap', fontdict={'fontsize':12}, pad=12);","metadata":{"execution":{"iopub.status.busy":"2022-05-03T12:12:33.052827Z","iopub.execute_input":"2022-05-03T12:12:33.054955Z","iopub.status.idle":"2022-05-03T12:12:34.520509Z","shell.execute_reply.started":"2022-05-03T12:12:33.054914Z","shell.execute_reply":"2022-05-03T12:12:34.519850Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"Because there appear to be no missing data or non-numerical values in jobs, there is no need for preprocessing when encoding and filling Nan rows, as there is with the IHDP dataset. Jobs, like IHDP, have a lot of outliers in the background variables, which requires a similar experiment with normalisation approach. However, we may investigate random forest regression models, which should manage any outliers internally and reduce the likelihood of our results being skewed or biassed in any way.","metadata":{}},{"cell_type":"code","source":"dfX.hist(bins=25,figsize=(14,10))\n","metadata":{"execution":{"iopub.status.busy":"2022-05-03T12:12:34.521744Z","iopub.execute_input":"2022-05-03T12:12:34.524178Z","iopub.status.idle":"2022-05-03T12:12:37.000528Z","shell.execute_reply.started":"2022-05-03T12:12:34.524135Z","shell.execute_reply":"2022-05-03T12:12:36.999851Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"the background variables in Jobs seem to be unbalanced","metadata":{}},{"cell_type":"code","source":"fig, axs = plt.subplots(1, 2, figsize=(16, 4))\nlimit = 20\naxs[0].scatter(jx[:, 0].reshape(-1, 1)[jt == 1][:limit]\n               , jy[jt == 1][:limit], label = \"Treated\")\naxs[0].scatter(jx[:, 0].reshape(-1, 1)[jt == 0][:limit]\n               , jy[jt == 0][:limit], label = \"Control\")\naxs[1].scatter(jx[:, 1].reshape(-1, 1)[jt == 1][:limit]\n               , jy[jt == 1][:limit], label = \"Treated\")\naxs[1].scatter(jx[:, 1].reshape(-1, 1)[jt == 0][:limit]\n               , jy[jt == 0][:limit], label = \"Control\")\naxs[0].legend(ncol=2)\naxs[1].legend(ncol=2)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-03T12:12:37.001966Z","iopub.execute_input":"2022-05-03T12:12:37.002435Z","iopub.status.idle":"2022-05-03T12:12:37.356502Z","shell.execute_reply.started":"2022-05-03T12:12:37.002395Z","shell.execute_reply":"2022-05-03T12:12:37.355848Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"In contrast to IHDP, jobsÂ outcomes are recorded as binary variables, therefore scatter points are only plotted on 0 and 1. Given the four graphs above, it's far more difficult to spot noticeable effects; yet, given the background variables we've chosen to depict, this could just be a coincidence.","metadata":{}},{"cell_type":"code","source":"fig, axs = plt.subplots(1, 2, figsize=(16, 4))\nlimit = 20\naxs[0].scatter(jx[:, 2].reshape(-1, 1)[jt == 1][:limit],\n               jy[jt == 1][:limit], label = \"Treated\")\naxs[0].scatter(jx[:, 2].reshape(-1, 1)[jt == 0][:limit]\n               , jy[jt == 0][:limit], label = \"Control\")\naxs[1].scatter(jx[:, 3].reshape(-1, 1)[jt == 1][:limit]\n               , jy[jt == 1][:limit], label = \"Treated\")\naxs[1].scatter(jx[:, 3].reshape(-1, 1)[jt == 0][:limit],\n               jy[jt == 0][:limit], label = \"Control\")\naxs[0].legend(ncol=2)\naxs[1].legend(ncol=2)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-03T12:12:37.357885Z","iopub.execute_input":"2022-05-03T12:12:37.358331Z","iopub.status.idle":"2022-05-03T12:12:37.700467Z","shell.execute_reply.started":"2022-05-03T12:12:37.358294Z","shell.execute_reply":"2022-05-03T12:12:37.699830Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"bins=20\nplt.hist(jt, bins=bins, color='hotpink')\nplt.title(\"Treatment and Control Distribution\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-03T12:12:37.701712Z","iopub.execute_input":"2022-05-03T12:12:37.702084Z","iopub.status.idle":"2022-05-03T12:12:37.907191Z","shell.execute_reply.started":"2022-05-03T12:12:37.702049Z","shell.execute_reply":"2022-05-03T12:12:37.906524Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"The graphs above demonstrate why we need to apply X-learner. In both datasets, there is an obvious imbalance in favour of the treatment and control groups; hopefully, X-learner will be able to account for this when calculating our CATE value.","metadata":{}},{"cell_type":"markdown","source":"### Data Modelling and Standardizing\n","metadata":{}},{"cell_type":"code","source":"jx_train, jx_test, jt_train, jt_test, jy_train, jy_test, je_train, je_test = train_test_split(jx, jt, jy, je, test_size=0.2)","metadata":{"execution":{"iopub.status.busy":"2022-05-03T12:12:37.908537Z","iopub.execute_input":"2022-05-03T12:12:37.908777Z","iopub.status.idle":"2022-05-03T12:12:37.914835Z","shell.execute_reply.started":"2022-05-03T12:12:37.908743Z","shell.execute_reply":"2022-05-03T12:12:37.914059Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"## Training Models - Standard Models","metadata":{}},{"cell_type":"markdown","source":"### Random Forest Regressor ","metadata":{}},{"cell_type":"code","source":"# Simple Random forest trained using the Jobs features + treatment\nrf_jobs = RandomForestRegressor() \nConc_train_j = np.concatenate([jx_train, jt_train], axis=1)\nrf_jobs.fit(Conc_train_j, jy_train.flatten())\n\n# predict Y0 and Y1 given t=0 and t=1 respectively for both datasets\n# t=0 - Jobs\nConcatenated_XT_zeros_Jobs = np.concatenate([jx_test, np.zeros_like(jt_test)], axis=1)\nj_y0 = rf_jobs.predict(Concatenated_XT_zeros_Jobs)\n# t=1 - Jobs\nConcatenated_XT_ones_Jobs = np.concatenate([jx_test, np.ones_like(jt_test)], axis=1)\nj_y1 = rf_jobs.predict(Concatenated_XT_ones_Jobs)\n\nestimated_eff = j_y1 - j_y0\nATT_Jobs = metrics.abs_att(estimated_eff, jy_test.flatten()\n                           , jt_test.flatten(), je_test.flatten())\nRpol = metrics.policy_risk(estimated_eff, jy_test.flatten()\n                           , jt_test.flatten(), je_test.flatten())\nprint( \"RF ATT, RF RISK POLICY :\", ATT_Jobs, Rpol )","metadata":{"execution":{"iopub.status.busy":"2022-05-03T12:12:37.916257Z","iopub.execute_input":"2022-05-03T12:12:37.916577Z","iopub.status.idle":"2022-05-03T12:12:38.773740Z","shell.execute_reply.started":"2022-05-03T12:12:37.916541Z","shell.execute_reply":"2022-05-03T12:12:38.772282Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"As discussed in the report, the Jobs dataset wil be evaluated using ATT and Rpol","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"temp_XJ = pd.DataFrame(jx_train)\ntemp_XJ_t = pd.DataFrame(jx_test)\n#temp_X_Jobs.head()\n#[temp_X_Jobs[cols].unique() for cols in temp_X_Jobs]\ntemp_XJ.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-03T12:12:38.774925Z","iopub.execute_input":"2022-05-03T12:12:38.775182Z","iopub.status.idle":"2022-05-03T12:12:38.798195Z","shell.execute_reply.started":"2022-05-03T12:12:38.775148Z","shell.execute_reply":"2022-05-03T12:12:38.797362Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"# Jobs\n# Scale columns 0,1,6,7,8,9,10,11,12,15 (all non binary)\ntemp_XJ.iloc[:, [0,1,6,7,8,9,10,11,12,15]] = StandardScaler().fit_transform(temp_XJ.iloc[:, [0,1,6,7,8,9,10,11,12,15]])\ntemp_XJ_t.iloc[:, [0,1,6,7,8,9,10,11,12,15]] = StandardScaler().fit_transform(temp_XJ_t.iloc[:, [0,1,6,7,8,9,10,11,12,15]]) \nJobs_xtrain_stan = temp_XJ.to_numpy()\nJobs_xtest_stan = temp_XJ_t.to_numpy()\ntemp_XJ.head()","metadata":{"execution":{"iopub.status.busy":"2022-05-03T12:12:38.799528Z","iopub.execute_input":"2022-05-03T12:12:38.800527Z","iopub.status.idle":"2022-05-03T12:12:38.837930Z","shell.execute_reply.started":"2022-05-03T12:12:38.800487Z","shell.execute_reply":"2022-05-03T12:12:38.837008Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":"### Random Forest Regressor - Optimized","metadata":{}},{"cell_type":"markdown","source":"10 fold cross validation Grid Search in combination with Random search","metadata":{}},{"cell_type":"markdown","source":"#### Grid Search","metadata":{}},{"cell_type":"code","source":"Benchmark_Jobs = RandomForestRegressor()\n# Our parameter Grid uses values centered around the parameters found from the random search\nparam_grid = {\n    'bootstrap': [True],\n    'max_depth': [10, 20, 30, 40, 50],\n    'max_features': ['auto', 'sqrt'],\n    'min_samples_leaf': [3, 4, 5],\n    'min_samples_split': [4, 5, 6],\n    'n_estimators': [600, 800, 1000]\n}\n\nGridSearch_RF_Jobs = GridSearchCV(estimator = Benchmark_Jobs, param_grid = param_grid, cv = 10, n_jobs = -1, verbose = 2)\nGridSearch_RF_Jobs.fit(Conc_train_j, jy_train.flatten())\nGridSearch_RF_Jobs.best_params_","metadata":{"execution":{"iopub.status.busy":"2022-05-03T12:12:38.839468Z","iopub.execute_input":"2022-05-03T12:12:38.839846Z","iopub.status.idle":"2022-05-03T14:27:38.969609Z","shell.execute_reply.started":"2022-05-03T12:12:38.839803Z","shell.execute_reply":"2022-05-03T14:27:38.968870Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":"#### Metric Evaluation","metadata":{}},{"cell_type":"code","source":"tuned_rf = RandomForestRegressor(bootstrap=True, max_depth=90, max_features='sqrt', min_samples_leaf=3, min_samples_split=6, n_estimators=800)\ntuned_rf.fit(Conc_train_j, jy_train.flatten())\n# t=0 \nConc_zeros_j = np.concatenate([jx_test, np.zeros_like(jt_test)], axis=1)\nY0_Jobs = tuned_rf.predict(Conc_zeros_j)\n# t=1 \nConc_ones_j = np.concatenate([jx_test, np.ones_like(jt_test)], axis=1)\nY1_Jobs = tuned_rf.predict(Conc_ones_j)\n#ITEs for Jobs dataset\ntuned_estimated_eff = Y1_Jobs - Y0_Jobs\nATT_Jobs_Optimized = metrics.abs_att(tuned_estimated_eff, jy_test.flatten(), jt_test.flatten(), je_test.flatten())\nRpol_Optimized = metrics.policy_risk(tuned_estimated_eff, jy_test.flatten(), jt_test.flatten(), je_test.flatten())\nprint('RF After hyperparameter tuning ATT, Risk Policy :', ATT_Jobs_Optimized, Rpol_Optimized)","metadata":{"execution":{"iopub.status.busy":"2022-05-03T14:27:38.971432Z","iopub.execute_input":"2022-05-03T14:27:38.971693Z","iopub.status.idle":"2022-05-03T14:27:41.888472Z","shell.execute_reply.started":"2022-05-03T14:27:38.971657Z","shell.execute_reply":"2022-05-03T14:27:41.887697Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":"From the metric table above, we can see no improvement on the accuracy of the effect predictions on the treated group; this being said we have a slight improvement in the models risk policy metric. The effect of the optimized model although isn't significant, could be effective for proving NSWP training affects a persons employability","metadata":{}},{"cell_type":"markdown","source":"### Feature Importances ","metadata":{}},{"cell_type":"code","source":"# Feature Importances for the Jobs dataset using random forest regression  \ntuned_rf.fit(Conc_train_j, jy_train.flatten())\nfeatures_Jobs = list(Conc_train_j)\nimportances = tuned_rf.feature_importances_\nstd_Jobs = np.std([tree.feature_importances_ for tree in tuned_rf.estimators_], axis=0)\nindices_Jobs_RF = np.argsort(importances)[::-1]\nprint(\"\")\nprint(\"Feature Importances - Random Forest\")\nfor f in range(Conc_train_j.shape[1]):\n    print(\"%d. %s (%f)\" % (f + 1, \"Feature Column: \"+ str(indices_Jobs_RF[f]), importances[indices_Jobs_RF[f]]))","metadata":{"execution":{"iopub.status.busy":"2022-05-03T14:27:41.889609Z","iopub.execute_input":"2022-05-03T14:27:41.889846Z","iopub.status.idle":"2022-05-03T14:27:44.453348Z","shell.execute_reply.started":"2022-05-03T14:27:41.889813Z","shell.execute_reply":"2022-05-03T14:27:44.452653Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"fig, axs = plt.subplots(1, 2, figsize=(16, 4))\naxs[0].bar(range(Conc_train_j.shape[1]), importances[indices_Jobs_RF], color=\"r\", yerr=std_Jobs[indices_Jobs_RF], align=\"center\")\naxs[0].set_xticks(range(Conc_train_j.shape[1]), indices_Jobs_RF)\naxs[0].set_xlim([-1, Conc_train_j.shape[1]])\naxs[0].set_ylim([0, None])\naxs[0].set_title(\"Feature Importances - Optimized Random Forest\", fontsize=12, fontweight=\"bold\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-05-03T14:27:44.454751Z","iopub.execute_input":"2022-05-03T14:27:44.455007Z","iopub.status.idle":"2022-05-03T14:27:44.791488Z","shell.execute_reply.started":"2022-05-03T14:27:44.454973Z","shell.execute_reply":"2022-05-03T14:27:44.790839Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"markdown","source":"## Propensity Score Re-weighting","metadata":{}},{"cell_type":"code","source":"simple_clf = RandomForestClassifier()\nsimple_clf.fit(jx_train, np.squeeze(jt_train))\n# These will act as our ptx values\nex_Jobs = simple_clf.predict_proba(jx_train).T[1].T + 0.0001","metadata":{"execution":{"iopub.status.busy":"2022-05-03T14:27:44.792893Z","iopub.execute_input":"2022-05-03T14:27:44.794161Z","iopub.status.idle":"2022-05-03T14:27:45.170950Z","shell.execute_reply.started":"2022-05-03T14:27:44.794121Z","shell.execute_reply":"2022-05-03T14:27:45.170254Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"markdown","source":"    \n$$w_i = \\frac{t_i}{e(x_i)} + \\frac{1-t_i}{1-e(x_i)}$$\n\n","metadata":{}},{"cell_type":"code","source":"import math\ndef Calc_Weights(ti, ex):\n    return (ti / ex) + ((1-ti) / (1-ex))","metadata":{"execution":{"iopub.status.busy":"2022-05-03T14:27:45.172345Z","iopub.execute_input":"2022-05-03T14:27:45.172611Z","iopub.status.idle":"2022-05-03T14:27:45.176651Z","shell.execute_reply.started":"2022-05-03T14:27:45.172575Z","shell.execute_reply":"2022-05-03T14:27:45.176047Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"Sample_Weights_Jobs = Calc_Weights(np.squeeze(jt_train), ex_Jobs)","metadata":{"execution":{"iopub.status.busy":"2022-05-03T14:27:45.177925Z","iopub.execute_input":"2022-05-03T14:27:45.178366Z","iopub.status.idle":"2022-05-03T14:27:45.188005Z","shell.execute_reply.started":"2022-05-03T14:27:45.178331Z","shell.execute_reply":"2022-05-03T14:27:45.187399Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"markdown","source":"Now that we have our sample weights, we can train our regressors using the weights","metadata":{}},{"cell_type":"code","source":"# Training variables for random forest \nConcatenated_XT_train_Jobs = np.concatenate([jx_train, jt_train], axis=1)\nRandomForest_Jobs_IPSW = RandomForestRegressor()\n# Trained regressors\nRandomForest_Jobs_IPSW.fit(Concatenated_XT_train_Jobs, jy_train.flatten(), sample_weight=Sample_Weights_Jobs)","metadata":{"execution":{"iopub.status.busy":"2022-05-03T14:27:45.189331Z","iopub.execute_input":"2022-05-03T14:27:45.189764Z","iopub.status.idle":"2022-05-03T14:27:46.043597Z","shell.execute_reply.started":"2022-05-03T14:27:45.189731Z","shell.execute_reply":"2022-05-03T14:27:46.042902Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"markdown","source":"#### Random forest Grid Search","metadata":{}},{"cell_type":"code","source":"# Our parameter Grid uses values centered around the parameters found from the random search\nparam_grid = {\n    'bootstrap': [True],\n    'max_depth': [40, 50, 60],\n    'max_features': ['auto', 'sqrt'],\n    'min_samples_leaf': [3, 4, 5],\n    'min_samples_split': [5, 7, 9],\n    'n_estimators': [400, 600, 800]\n}\n\nGridSearch_RF_Jobs_IPSW = GridSearchCV(estimator = RandomForest_Jobs_IPSW, param_grid = param_grid, cv = 10, n_jobs = -1, verbose = 2)\nGridSearch_RF_Jobs_IPSW.fit(Concatenated_XT_train_Jobs, jy_train.flatten(), sample_weight=Sample_Weights_Jobs)\nGridSearch_RF_Jobs_IPSW.best_params_","metadata":{"execution":{"iopub.status.busy":"2022-05-03T14:27:46.044898Z","iopub.execute_input":"2022-05-03T14:27:46.045303Z","iopub.status.idle":"2022-05-03T15:29:59.144745Z","shell.execute_reply.started":"2022-05-03T14:27:46.045265Z","shell.execute_reply":"2022-05-03T15:29:59.144005Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"markdown","source":"#### Metric Evaluation","metadata":{}},{"cell_type":"code","source":"RF_IPSW_Jobs = RandomForestRegressor(bootstrap=True, max_depth=60, max_features='sqrt', min_samples_leaf=4, min_samples_split=9, n_estimators=600)\nRF_IPSW_Jobs.fit(Conc_train_j, jy_train.flatten(), sample_weight=Sample_Weights_Jobs)\n# t=0 - Jobs\nConcatenated_XT_zeros_Jobs = np.concatenate([jx_test, np.zeros_like(jt_test)], axis=1)\nY0_Jobs_IPSW_RF = RF_IPSW_Jobs.predict(Concatenated_XT_zeros_Jobs)\n# t=1 - Jobs\nConcatenated_XT_ones_Jobs = np.concatenate([jx_test, np.ones_like(jt_test)], axis=1)\nY1_Jobs_IPSW_RF = RF_IPSW_Jobs.predict(Concatenated_XT_ones_Jobs)\n#ITEs for Random Forest Model Jobs IPSW\nEffect_Estimates_Jobs_RF_IPSW = Y1_Jobs_IPSW_RF - Y0_Jobs_IPSW_RF\n# Metric calculations for IPSW\nATT_Jobs_RF_IPSW = metrics.abs_att(Effect_Estimates_Jobs_RF_IPSW, jy_test.flatten(), jt_test.flatten(), je_test.flatten())\nRpol_RF_IPSW     = metrics.policy_risk(Effect_Estimates_Jobs_RF_IPSW, jy_test.flatten(), jt_test.flatten(), je_test.flatten())\nprint('IPSW Random Forest ATT, Risk Policy :',ATT_Jobs_RF_IPSW ,Rpol_RF_IPSW)","metadata":{"execution":{"iopub.status.busy":"2022-05-03T15:29:59.148721Z","iopub.execute_input":"2022-05-03T15:29:59.148937Z","iopub.status.idle":"2022-05-03T15:30:01.391555Z","shell.execute_reply.started":"2022-05-03T15:29:59.148910Z","shell.execute_reply":"2022-05-03T15:30:01.390706Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"markdown","source":"The propensity score re-weighting has no noticeable impact on prediction performance. The error readings for the optimised linear regressor are still the lowest. Now we can see how the feature importances were influenced by the propensity score weighting. We see a rise in the significance of the treatment feature.","metadata":{}},{"cell_type":"markdown","source":"### Feature Importances for IPSW ","metadata":{}},{"cell_type":"code","source":"from matplotlib import pyplot\n\n# Feature Importances for the Jobs dataset using random forest regression  \nfeatures_Jobs = list(Conc_train_j)\nimportances = RF_IPSW_Jobs.feature_importances_\nstd_Jobs = np.std([tree.feature_importances_ for tree in RF_IPSW_Jobs.estimators_], axis=0)\nindices_Jobs_RF = np.argsort(importances)[::-1]\nprint(\"\")\nprint(\"Feature Importances - RF IPSW\")\nfor f in range(Conc_train_j.shape[1]):\n    print(\"%d. %s (%f)\" % (f + 1, \"Feature Column: \"+ str(indices_Jobs_RF[f]), importances[indices_Jobs_RF[f]]))\npyplot.bar([x for x in range(len(importances))], importances)\npyplot.title(\"Feature Importance for Jobs dataset using RF\")\npyplot.ylabel(\"Feature Score\")\npyplot.xlabel(\"Feature Number\")\npyplot.show()\n    ","metadata":{"execution":{"iopub.status.busy":"2022-05-03T15:30:01.392793Z","iopub.execute_input":"2022-05-03T15:30:01.393092Z","iopub.status.idle":"2022-05-03T15:30:01.703563Z","shell.execute_reply.started":"2022-05-03T15:30:01.393054Z","shell.execute_reply":"2022-05-03T15:30:01.702867Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"markdown","source":"## Task 5 CATE Estimators","metadata":{}},{"cell_type":"markdown","source":"Due to its performance, we will be using random forest as the base learners for XL","metadata":{}},{"cell_type":"code","source":"xl_Jobs = XLearner(models=RandomForestRegressor(), propensity_model=RandomForestClassifier())\nxl_Jobs.fit(jy_train, jt_train.flatten(), X=jx_train)\nxl_te_test_Jobs = xl_Jobs.effect(jx_test)","metadata":{"execution":{"iopub.status.busy":"2022-05-03T15:30:01.704902Z","iopub.execute_input":"2022-05-03T15:30:01.705177Z","iopub.status.idle":"2022-05-03T15:30:04.547531Z","shell.execute_reply.started":"2022-05-03T15:30:01.705142Z","shell.execute_reply":"2022-05-03T15:30:04.546601Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"markdown","source":"Calculating our ATE and PEHE metrics ","metadata":{}},{"cell_type":"code","source":"xl_att_test = metrics.abs_att(xl_te_test_Jobs, jy_test.flatten(), jt_test.flatten(), je_test.flatten())\nxl_rpol = metrics.policy_risk(xl_te_test_Jobs.flatten(), jy_test.flatten(), jt_test.flatten(), je_test.flatten())","metadata":{"execution":{"iopub.status.busy":"2022-05-03T15:30:04.548822Z","iopub.execute_input":"2022-05-03T15:30:04.549099Z","iopub.status.idle":"2022-05-03T15:30:04.555083Z","shell.execute_reply.started":"2022-05-03T15:30:04.549065Z","shell.execute_reply":"2022-05-03T15:30:04.554271Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"# Metrics for Jobs Dataset\nresults_ = []\nresults_.append(['Random Forest', ATT_Jobs, Rpol])\nresults_.append(['Hyperparameter tuned Random Forest', ATT_Jobs_Optimized, Rpol_Optimized])\nresults_.append(['IPSW Random Forest',ATT_Jobs_RF_IPSW ,Rpol_RF_IPSW])\nresults_.append(['X-Learner',xl_att_test ,xl_rpol])\ncols_ = ['Methodology', 'ATT test', 'Risk Policy']\ndf_Second = pd.DataFrame(results_, columns=cols_)\ndf_Second","metadata":{"execution":{"iopub.status.busy":"2022-05-03T15:30:04.556956Z","iopub.execute_input":"2022-05-03T15:30:04.557196Z","iopub.status.idle":"2022-05-03T15:30:04.578757Z","shell.execute_reply.started":"2022-05-03T15:30:04.557164Z","shell.execute_reply":"2022-05-03T15:30:04.577931Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"markdown","source":"It seems that the best performing model for this optimised dataset is the standard random forest; having not only the most accurate average treatment effect on the treated, but also a fairly low risk policy. ","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}